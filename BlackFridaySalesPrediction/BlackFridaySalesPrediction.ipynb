{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import numpy as np\r\n",
    "import umap\r\n",
    "import dill\r\n",
    "import featuretools as ft\r\n",
    "import lightgbm as lgb\r\n",
    "import janitor\r\n",
    "\r\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, RobustScaler\r\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\r\n",
    "from pyod.models.copod import COPOD\r\n",
    "\r\n",
    "from sklearn.experimental import enable_iterative_imputer\r\n",
    "from sklearn.impute import IterativeImputer\r\n",
    "from sklearn.linear_model import ElasticNet\r\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\r\n",
    "from sklearn.inspection import permutation_importance\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Used only for saving/loading session variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#dill.dump_session(\"session.db\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#dill.load_session(\"session.db\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\r\n",
    "data_test = pd.read_csv(\"test.csv\")\r\n",
    "\r\n",
    "data_submission = pd.DataFrame({\"User_ID\" : data_test[\"User_ID\"], \"Product_ID\" : data_test[\"Product_ID\"]})\r\n",
    "data_train.drop([\"User_ID\", \"Product_ID\"], axis=1, inplace=True)\r\n",
    "data_test.drop([\"User_ID\", \"Product_ID\"], axis=1, inplace=True)\r\n",
    "data_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"\"\"Train NaN: \r\n",
    "{data_train.isnull().mean() * 100}\r\n",
    "\r\n",
    "Test NaN: \r\n",
    "{data_test.isnull().mean() * 100}\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train[\"Product_Category_2\"] = data_train[\"Product_Category_2\"].fillna(data_train[\"Product_Category_2\"].median()).astype(\"object\")\r\n",
    "data_train.drop(\"Product_Category_3\", axis=1, inplace=True)\r\n",
    "data_test[\"Product_Category_2\"] = data_test[\"Product_Category_2\"].fillna(data_test[\"Product_Category_2\"].median()).astype(\"object\")\r\n",
    "data_test.drop(\"Product_Category_3\", axis=1, inplace=True)\r\n",
    "data_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = data_train.pop(\"Purchase\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_cols = list(data_train.select_dtypes(include=\"object\"))\r\n",
    "oe = OrdinalEncoder()\r\n",
    "for col in cat_cols:\r\n",
    "    data_train[col] = oe.fit_transform(data_train[col].values.reshape(-1, 1))\r\n",
    "    data_test[col] = oe.transform(data_test[col].values.reshape(-1, 1))\r\n",
    "data_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def comb_prod_cat(df): # Combine product category\r\n",
    "    prod_cats = [\"Product_Category_1\", \"Product_Category_2\"]\r\n",
    "    df[\"Total_Category\"] = df[prod_cats[0]] + df[prod_cats[1]]\r\n",
    "    return df.drop(prod_cats, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train = comb_prod_cat(data_train)\r\n",
    "data_test = comb_prod_cat(data_test)\r\n",
    "data_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(27, 9.5))\r\n",
    "r_idx = 0\r\n",
    "c_idx = 0\r\n",
    "for col in data_train:\r\n",
    "    axes[r_idx][c_idx].set_title(f\"{col} Distribution\")\r\n",
    "    data_train[col].hist(ax=axes[r_idx][c_idx])\r\n",
    "    c_idx += 1\r\n",
    "    if c_idx >= 4:\r\n",
    "        r_idx += 1\r\n",
    "        c_idx = 0\r\n",
    "axes[r_idx][c_idx].set_title(\"Purchase (Target) Distribution\")\r\n",
    "axes[1][3] = y.hist(ax=axes[1][3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "umap_model = umap.UMAP(n_components=2, n_neighbors=100, min_dist=0.01, n_jobs=-1, random_state=0)\r\n",
    "sample_umap = pd.DataFrame(data=MinMaxScaler().fit_transform(data_train.sample(frac=0.1)), columns=data_train.columns)\r\n",
    "data_umap = umap_model.fit_transform(sample_umap)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 8))\r\n",
    "ax.set_xlabel(\"X\")\r\n",
    "ax.set_ylabel(\"Y\")\r\n",
    "ax.set_title(\"UMAP Dimensionality Reduced data_train; the redder the higher target value\")\r\n",
    "ax.scatter(data_umap[:,0], data_umap[:,1], c=y.loc[sample.index], marker=\".\", linewidths=1.75, cmap=\"YlOrRd\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_vif = data_train.sample(frac=0.1)\r\n",
    "vif_factors = [variance_inflation_factor(sample_vif.values, i) for i in range(sample_vif.shape[1])]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(29, 8))\r\n",
    "ax.set_title(\"Variance Inflation Factor (Multicollinearity)\")\r\n",
    "pd.DataFrame({\"Feature\" : data_train.columns, \"VIF\" : vif_factors}).sort_values(by=\"VIF\", ascending=False).plot.bar(x=\"Feature\", y=\"VIF\", ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_model = COPOD(contamination=0.01, n_jobs=-1)\r\n",
    "out_model.fit(data_train)\r\n",
    "out_indices = pd.Series(np.argwhere(out_model.labels_ == 1).flatten())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 8))\r\n",
    "ax.set_xlabel(\"X\")\r\n",
    "ax.set_ylabel(\"Y\")\r\n",
    "ax.set_title(\"UMAP Dimensionality Reduced data_train; blue indicates inlier, orange indicates top 1 percent outliers\")\r\n",
    "ax.scatter(data_umap[:,0], data_umap[:,1], marker=\".\", linewidths=1)\r\n",
    "data_x = pd.Series(data_umap[:,0])\r\n",
    "data_y = pd.Series(data_umap[:,1])\r\n",
    "ax.scatter(data_x[data_x.index.isin(out_indices)], data_y[data_y.index.isin(out_indices)], marker=\".\", linewidths=1, cmap=\"YlOrRd\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 7))\r\n",
    "sns.histplot(pd.DataFrame({\"Outlier Score\" : out_model.decision_scores_, \"Outlier\" : out_model.labels_}), x=\"Outlier Score\", hue=\"Outlier\", bins=225, multiple=\"stack\", ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "drop_idx = data_train.index[out_indices]\r\n",
    "data_train = data_train.drop(out_indices, axis=0).reset_index(drop=True) # Remove top outliers\r\n",
    "y = y.drop(drop_idx, axis=0).astype(\"float64\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train[\"ID\"] = pd.Series(np.arange(0, data_train.shape[0]))\r\n",
    "data_test[\"ID\"] = pd.Series(np.arange(0, data_test.shape[0]))\r\n",
    "es_train = ft.EntitySet(id=\"sales\")\r\n",
    "es_train = es_train.entity_from_dataframe(entity_id=\"sales\", dataframe=data_train, index=\"ID\")\r\n",
    "es_test = ft.EntitySet(id=\"sales\")\r\n",
    "es_test = es_test.entity_from_dataframe(entity_id=\"sales\", dataframe=data_test, index=\"ID\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "primitives = ft.list_primitives()\r\n",
    "agg_num_arr = ((primitives[\"type\"] == \"aggregation\") & (primitives[\"valid_inputs\"] == \"Numeric\"))\r\n",
    "trans_num_arr = ((primitives[\"type\"] == \"transform\") & (primitives[\"valid_inputs\"] == \"Numeric\"))\r\n",
    "num_agg_prim = list(primitives[agg_num_arr][\"name\"]) # Aggregation primitives whose valid input is numeric\r\n",
    "num_trans_prim = list(primitives[trans_num_arr][\"name\"]) # Transform primitives whose valid input is numeric"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_features(es):\r\n",
    "    features, feature_names = ft.dfs(entityset=es, target_entity=\"sales\", agg_primitives=num_agg_prim, trans_primitives=num_trans_prim, max_depth=1, max_features=500)\r\n",
    "    features.replace([np.inf, -np.inf], np.nan, inplace=True) \r\n",
    "    return features, feature_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_train, _ = generate_features(es_train)\r\n",
    "features_test, _ = generate_features(es_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "remove_nan_threshold = 0.3\r\n",
    "features_train = features_train.loc[:, features_train.isnull().mean() < remove_nan_threshold]\r\n",
    "features_test = features_test.loc[:, features_test.isnull().mean() < remove_nan_threshold]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ii = IterativeImputer(estimator=ElasticNet(normalize=True, random_state=0))\r\n",
    "features_train = pd.DataFrame(data=ii.fit_transform(features_train), columns=features_train.columns)\r\n",
    "features_test = pd.DataFrame(data=ii.transform(features_test), columns=features_test.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rs = RobustScaler()\r\n",
    "features_train_scaled = pd.DataFrame(data=rs.fit_transform(features_train), columns=features_train.columns)\r\n",
    "features_test_scaled = pd.DataFrame(data=rs.transform(features_test), columns=features_test.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_col(name):\r\n",
    "    return (\r\n",
    "        str(name).strip().lower().replace(\"<\", \"\").replace(\">\", \"\").replace(\":\", \"\").replace(\"feature\", \"\")\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_train_scaled = features_train_scaled.rename(columns=clean_col)\r\n",
    "features_test_scaled = features_test_scaled.rename(columns=clean_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgbm_baseline_params = { # LGBM regressor baseline (default) estimator parameters\r\n",
    "    \"num_iterations\" : 100,\r\n",
    "    \"learning_rate\" : 0.05,\r\n",
    "    \"max_depth\" : 6,\r\n",
    "    \"num_leaves\" : 255,\r\n",
    "    \"random_state\" : 0\r\n",
    "}\r\n",
    "lgbm_feature_importance = lgb.LGBMRegressor(**lgbm_baseline_params)\r\n",
    "lgbm_feature_importance.fit(features_train_scaled, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(27, 45))\r\n",
    "lgb.plot_importance(lgbm_feature_importance, ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ee_feature_importance = ElasticNet(random_state=0)\r\n",
    "ee_feature_importance.fit(features_train_scaled, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(27, 45))\r\n",
    "ax.set_title(\"Feature importance\")\r\n",
    "list_coef, list_cols = [list(tuple) for tuple in zip(*sorted(zip(np.abs(ee_feature_importance.coef_), features_train_scaled.columns), reverse=False))]\r\n",
    "pd.Series(data=list_coef, index=list_cols).plot.barh(ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feat_imp_combined = (np.abs((lgbm_feature_importance.feature_importances_ + ee_feature_importance.coef_)) / 2).flatten()\r\n",
    "fig, ax = plt.subplots(figsize=(27, 45))\r\n",
    "ax.set_title(\"Feature importance (LGBM feature importance and ElasticNet averaged)\")\r\n",
    "list_coef, list_cols = [list(tuple) for tuple in zip(*sorted(zip(feat_imp_combined, features_train_scaled.columns), reverse=False))]\r\n",
    "pd.Series(data=list_coef, index=list_cols).plot.barh(ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgbm_baseline_scores = cross_val_score(estimator=lgbm_baseline_estimator, X=features_train_scaled, y=y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"\"\"LGBM cross-validated RMSE baseline score\r\n",
    "{np.mean(lgbm_baseline_scores)}\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgbm_baseline_estimator.fit(features_train_scaled, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "perm_result = permutation_importance(estimator=lgbm_baseline_estimator, X=features_train_scaled, y=y, n_repeats=3, scoring=\"neg_root_mean_squared_error\", n_jobs=3, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(27, 45))\r\n",
    "ax.set_title(\"Permutation importance\")\r\n",
    "list_coef, list_cols = [list(tuple) for tuple in zip(*sorted(zip(perm_result.importances_mean, features_train_scaled.columns), reverse=False))]\r\n",
    "pd.Series(data=list_coef, index=list_cols).plot.barh(ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_columns = [y for x, y in zip(list_coef, list_cols) if x > 0] # Drop columns which did not contribute in permutation score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = features_train_scaled[selected_columns] # Final data which is cleaned, feature engineered, scaled, feature selected, etc.\r\n",
    "X_test = features_test_scaled[selected_columns]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgbm_reg = lgb.LGBMRegressor(**lgbm_baseline_params) # Final estimator object"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgbm_after_feat_select = cross_val_score(estimator=lgbm_reg, X=X_train, y=y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"\"\"LGBM cross-validated RMSE after feature selection score\r\n",
    "{np.mean(lgbm_after_feat_select)}\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {\r\n",
    "    \"num_iterations\" : [100, 200, 300],\r\n",
    "    \"learning_rate\" : [0.01, 0.02, 0.03],\r\n",
    "    \"max_depth\" : [6, 9, 12],\r\n",
    "    \"num_leaves\" : [63, 127, 255],\r\n",
    "    \"random_state\" : [0]\r\n",
    "}\r\n",
    "gs_cv = GridSearchCV(estimator=lgbm_reg, param_grid=params, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\r\n",
    "gs_cv.fit(X_train, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"\"\"GridSearchCV results\r\n",
    "Best Params: {gs_cv.best_params_}\r\n",
    "Best Score: {gs_cv.best_score_}\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lgbm_reg.set_params(**gs_cv.best_params_)\r\n",
    "lgbm_reg.fit(X_train, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_submission[\"Purchase\"] = lgbm_reg.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_submission.to_csv(\"results.csv\", index=False, header=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}